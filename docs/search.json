[
  {
    "objectID": "notebooks/project.html",
    "href": "notebooks/project.html",
    "title": "Project",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\nimport voila\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive, fixed, interact_manual"
  },
  {
    "objectID": "notebooks/project.html#modules-import",
    "href": "notebooks/project.html#modules-import",
    "title": "Project",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\nimport voila\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive, fixed, interact_manual"
  },
  {
    "objectID": "notebooks/project.html#loading-datasets",
    "href": "notebooks/project.html#loading-datasets",
    "title": "Project",
    "section": "Loading Datasets",
    "text": "Loading Datasets\n\nFILENAME= \"calendar.csv\"\nDATADIR = \"../rohlik_project/csv/\"\n\ncalendar_df = pd.read_csv(DATADIR + FILENAME, header=0)\n\n\nFILENAME= \"inventory.csv\"\nDATADIR = \"../rohlik_project/csv/\"\n\ninventory_df = pd.read_csv(DATADIR + FILENAME, header=0)\n\n\nFILENAME= \"sales_test.csv\"\nDATADIR = \"../rohlik_project/csv/\"\n\nsales_test_df = pd.read_csv(DATADIR + FILENAME, header=0)\n\n\nFILENAME= \"sales_train.csv\"\nDATADIR = \"../rohlik_project/csv/\"\n\nsales_train_df = pd.read_csv(DATADIR + FILENAME, low_memory=False)\n\n\nFILENAME= \"solution.csv\"\nDATADIR = \"../rohlik_project/csv/\"\n\nsolution_df = pd.read_csv(DATADIR + FILENAME)\n\n\nFILENAME= \"test_weights.csv\"\nDATADIR = \"../rohlik_project/csv/\"\n\ntest_weights_df = pd.read_csv(DATADIR + FILENAME)"
  },
  {
    "objectID": "notebooks/project.html#information-about-datasets",
    "href": "notebooks/project.html#information-about-datasets",
    "title": "Project",
    "section": "Information about datasets",
    "text": "Information about datasets\n\ncalendar_df\n\ncalendar_df.head()\n\n\n\n\n\n\n\n\ndate\nholiday_name\nholiday\nshops_closed\nwinter_school_holidays\nschool_holidays\nwarehouse\n\n\n\n\n0\n2022-03-16\nNaN\n0\n0\n0\n0\nFrankfurt_1\n\n\n1\n2020-03-22\nNaN\n0\n0\n0\n0\nFrankfurt_1\n\n\n2\n2018-02-07\nNaN\n0\n0\n0\n0\nFrankfurt_1\n\n\n3\n2018-08-10\nNaN\n0\n0\n0\n0\nFrankfurt_1\n\n\n4\n2017-10-26\nNaN\n0\n0\n0\n0\nPrague_2\n\n\n\n\n\n\n\n\ncalendar_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 23016 entries, 0 to 23015\nData columns (total 7 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   date                    23016 non-null  object\n 1   holiday_name            930 non-null    object\n 2   holiday                 23016 non-null  int64 \n 3   shops_closed            23016 non-null  int64 \n 4   winter_school_holidays  23016 non-null  int64 \n 5   school_holidays         23016 non-null  int64 \n 6   warehouse               23016 non-null  object\ndtypes: int64(4), object(3)\nmemory usage: 1.2+ MB\n\n\n\ndisplay(calendar_df.date.min())\ndisplay(calendar_df.date.max())\n\n'2016-01-01'\n\n\n'2024-12-31'\n\n\nThe earliest and the latest date in the dataset.\n\n\ninventory_df\n\ninventory_df.head()\n\n\n\n\n\n\n\n\nunique_id\nproduct_unique_id\nname\nL1_category_name_en\nL2_category_name_en\nL3_category_name_en\nL4_category_name_en\nwarehouse\n\n\n\n\n0\n5255\n2583\nPastry_196\nBakery\nBakery_L2_14\nBakery_L3_26\nBakery_L4_1\nPrague_3\n\n\n1\n4948\n2426\nHerb_19\nFruit and vegetable\nFruit and vegetable_L2_30\nFruit and vegetable_L3_86\nFruit and vegetable_L4_1\nPrague_3\n\n\n2\n2146\n1079\nBeet_2\nFruit and vegetable\nFruit and vegetable_L2_3\nFruit and vegetable_L3_65\nFruit and vegetable_L4_34\nPrague_1\n\n\n3\n501\n260\nChicken_13\nMeat and fish\nMeat and fish_L2_13\nMeat and fish_L3_27\nMeat and fish_L4_5\nPrague_1\n\n\n4\n4461\n2197\nChicory_1\nFruit and vegetable\nFruit and vegetable_L2_17\nFruit and vegetable_L3_33\nFruit and vegetable_L4_1\nFrankfurt_1\n\n\n\n\n\n\n\n\ninventory_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5432 entries, 0 to 5431\nData columns (total 8 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   unique_id            5432 non-null   int64 \n 1   product_unique_id    5432 non-null   int64 \n 2   name                 5432 non-null   object\n 3   L1_category_name_en  5432 non-null   object\n 4   L2_category_name_en  5432 non-null   object\n 5   L3_category_name_en  5432 non-null   object\n 6   L4_category_name_en  5432 non-null   object\n 7   warehouse            5432 non-null   object\ndtypes: int64(2), object(6)\nmemory usage: 339.6+ KB\n\n\n\n\nsales_train_df\n\nsales_train_df.head()\n\n\n\n\n\n\n\n\nunique_id\ndate\nwarehouse\ntotal_orders\nsales\nsell_price_main\navailability\ntype_0_discount\ntype_1_discount\ntype_2_discount\ntype_3_discount\ntype_4_discount\ntype_5_discount\ntype_6_discount\n\n\n\n\n0\n4845\n2024-03-10\nBudapest_1\n6436.0\n16.34\n646.26\n1.00\n0.00000\n0.0\n0.0\n0.0\n0.15312\n0.0\n0.0\n\n\n1\n4845\n2021-05-25\nBudapest_1\n4663.0\n12.63\n455.96\n1.00\n0.00000\n0.0\n0.0\n0.0\n0.15025\n0.0\n0.0\n\n\n2\n4845\n2021-12-20\nBudapest_1\n6507.0\n34.55\n455.96\n1.00\n0.00000\n0.0\n0.0\n0.0\n0.15025\n0.0\n0.0\n\n\n3\n4845\n2023-04-29\nBudapest_1\n5463.0\n34.52\n646.26\n0.96\n0.20024\n0.0\n0.0\n0.0\n0.15312\n0.0\n0.0\n\n\n4\n4845\n2022-04-01\nBudapest_1\n5997.0\n35.92\n486.41\n1.00\n0.00000\n0.0\n0.0\n0.0\n0.15649\n0.0\n0.0\n\n\n\n\n\n\n\n\nsales_train_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1048575 entries, 0 to 1048574\nData columns (total 14 columns):\n #   Column           Non-Null Count    Dtype  \n---  ------           --------------    -----  \n 0   unique_id        1048575 non-null  int64  \n 1   date             1048575 non-null  object \n 2   warehouse        1048575 non-null  object \n 3   total_orders     1048556 non-null  float64\n 4   sales            1048556 non-null  float64\n 5   sell_price_main  1048575 non-null  float64\n 6   availability     1048575 non-null  float64\n 7   type_0_discount  1048575 non-null  float64\n 8   type_1_discount  1048575 non-null  float64\n 9   type_2_discount  1048575 non-null  float64\n 10  type_3_discount  1048575 non-null  float64\n 11  type_4_discount  1048575 non-null  float64\n 12  type_5_discount  1048575 non-null  float64\n 13  type_6_discount  1048575 non-null  float64\ndtypes: float64(11), int64(1), object(2)\nmemory usage: 112.0+ MB\n\n\n\ndisplay(sales_train_df.date.min())\ndisplay(sales_train_df.date.max())\n\n'2020-08-01'\n\n\n'2024-06-02'\n\n\nThe earliest and the latest date in the dataset.\n\n\nsales_test_df\n\nsales_test_df.head(15)\n\n\n\n\n\n\n\n\nunique_id\ndate\nwarehouse\ntotal_orders\nsell_price_main\ntype_0_discount\ntype_1_discount\ntype_2_discount\ntype_3_discount\ntype_4_discount\ntype_5_discount\ntype_6_discount\n\n\n\n\n0\n1226\n2024-06-03\nBrno_1\n8679.0\n13.13\n0.00000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n1226\n2024-06-11\nBrno_1\n8795.0\n13.13\n0.15873\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1226\n2024-06-13\nBrno_1\n10009.0\n13.13\n0.15873\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n1226\n2024-06-15\nBrno_1\n8482.0\n13.13\n0.15873\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n1226\n2024-06-09\nBrno_1\n8195.0\n13.13\n0.00000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n5\n1226\n2024-06-06\nBrno_1\n9538.0\n13.13\n0.00000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n6\n1226\n2024-06-12\nBrno_1\n9002.0\n13.13\n0.15873\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n7\n1226\n2024-06-05\nBrno_1\n8501.0\n13.13\n0.00000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n8\n1226\n2024-06-04\nBrno_1\n8492.0\n13.13\n0.00000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n9\n1226\n2024-06-08\nBrno_1\n8093.0\n13.13\n0.00000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10\n1226\n2024-06-07\nBrno_1\n10174.0\n13.13\n0.00000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n11\n1226\n2024-06-14\nBrno_1\n10363.0\n13.13\n0.15873\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n12\n1226\n2024-06-16\nBrno_1\n8453.0\n13.13\n0.15873\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n13\n1226\n2024-06-10\nBrno_1\n9275.0\n13.13\n0.15873\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n14\n5409\n2024-06-08\nPrague_2\n5533.0\n58.26\n0.00000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nsales_test_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 47021 entries, 0 to 47020\nData columns (total 12 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   unique_id        47021 non-null  int64  \n 1   date             47021 non-null  object \n 2   warehouse        47021 non-null  object \n 3   total_orders     47021 non-null  float64\n 4   sell_price_main  47021 non-null  float64\n 5   type_0_discount  47021 non-null  float64\n 6   type_1_discount  47021 non-null  float64\n 7   type_2_discount  47021 non-null  float64\n 8   type_3_discount  47021 non-null  float64\n 9   type_4_discount  47021 non-null  float64\n 10  type_5_discount  47021 non-null  float64\n 11  type_6_discount  47021 non-null  float64\ndtypes: float64(9), int64(1), object(2)\nmemory usage: 4.3+ MB\n\n\n\ndisplay(sales_test_df.date.min())\ndisplay(sales_test_df.date.max())\n\n'2024-06-03'\n\n\n'2024-06-16'\n\n\nThe earliest and the latest date in the dataset.\n\nsolution_df\n\nsolution_df.head(5)\n\n\n\n\n\n\n\n\nid\nsales_hat\n\n\n\n\n0\nBrno_1_1226_2024-06-03\n242.320371\n\n\n1\nBrno_1_1226_2024-06-11\n232.836769\n\n\n2\nBrno_1_1226_2024-06-13\n255.140114\n\n\n3\nBrno_1_1226_2024-06-15\n233.518247\n\n\n4\nBrno_1_1226_2024-06-09\n227.516824\n\n\n\n\n\n\n\n\nsolution_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 47021 entries, 0 to 47020\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   id         47021 non-null  object \n 1   sales_hat  47021 non-null  float64\ndtypes: float64(1), object(1)\nmemory usage: 734.8+ KB\n\n\n\n\ntest_weight_df\n\ntest_weights_df.head(5)\n\n\n\n\n\n\n\n\nunique_id\nweight\n\n\n\n\n0\n0\n2.535369\n\n\n1\n1\n3.888933\n\n\n2\n2\n5.885237\n\n\n3\n3\n2.139552\n\n\n4\n5\n3.021715\n\n\n\n\n\n\n\n\ntest_weights_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5390 entries, 0 to 5389\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   unique_id  5390 non-null   int64  \n 1   weight     5390 non-null   float64\ndtypes: float64(1), int64(1)\nmemory usage: 84.3 KB"
  },
  {
    "objectID": "notebooks/project.html#explanatory-data-analysis",
    "href": "notebooks/project.html#explanatory-data-analysis",
    "title": "Project",
    "section": "Explanatory Data Analysis",
    "text": "Explanatory Data Analysis\n\n# Merging\ndata = sales_train_df.merge(calendar_df, on='date', how='left')\ndisplay(data.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 7340025 entries, 0 to 7340024\nData columns (total 20 columns):\n #   Column                  Dtype  \n---  ------                  -----  \n 0   unique_id               int64  \n 1   date                    object \n 2   warehouse_x             object \n 3   total_orders            float64\n 4   sales                   float64\n 5   sell_price_main         float64\n 6   availability            float64\n 7   type_0_discount         float64\n 8   type_1_discount         float64\n 9   type_2_discount         float64\n 10  type_3_discount         float64\n 11  type_4_discount         float64\n 12  type_5_discount         float64\n 13  type_6_discount         float64\n 14  holiday_name            object \n 15  holiday                 int64  \n 16  shops_closed            int64  \n 17  winter_school_holidays  int64  \n 18  school_holidays         int64  \n 19  warehouse_y             object \ndtypes: float64(11), int64(5), object(4)\nmemory usage: 1.1+ GB\n\n\nNone\n\n\n\n# Dropping redundant column\ndata=data.drop(columns=['warehouse_y'])\ndata=data.rename(columns={'warehouse_x':'warehouse'})\ndisplay(data.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 7340025 entries, 0 to 7340024\nData columns (total 19 columns):\n #   Column                  Dtype  \n---  ------                  -----  \n 0   unique_id               int64  \n 1   date                    object \n 2   warehouse               object \n 3   total_orders            float64\n 4   sales                   float64\n 5   sell_price_main         float64\n 6   availability            float64\n 7   type_0_discount         float64\n 8   type_1_discount         float64\n 9   type_2_discount         float64\n 10  type_3_discount         float64\n 11  type_4_discount         float64\n 12  type_5_discount         float64\n 13  type_6_discount         float64\n 14  holiday_name            object \n 15  holiday                 int64  \n 16  shops_closed            int64  \n 17  winter_school_holidays  int64  \n 18  school_holidays         int64  \ndtypes: float64(11), int64(5), object(3)\nmemory usage: 1.1+ GB\n\n\nNone\n\n\n\n# Data Cleaning and Feature Engineering\n# Convert 'date' column to datetime\ndata['date'] = pd.to_datetime(data['date'])\n\n# Add 'year' and 'month' columns\ndata['year'] = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata['day'] = data['date'].dt.day\ndata['day_of_week'] = pd.to_datetime(data['date']).dt.dayofweek\ndata['weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\ndata['holiday'] = data['holiday'].fillna(0).astype(int)\n\n# Changing integer values to string\ndata['day_of_week'] = data['day_of_week'].astype('string')\ndata['holiday'] = data['holiday'].astype('string')\n\n# Changing numbers to day of week names, weekend, and holiday\nday_map = {'0': 'Monday','1': 'Tuesday', '2': 'Wednesday', '3': 'Thursday', '4': 'Friday', '5': 'Saturday', '6': 'Sunday'}\ndata['day_of_week'] = data['day_of_week'].map(day_map)\nholiday_map={'0': 'Not  a Holiday', '1':'Holiday'}\ndata['holiday'] = data['holiday'].map(holiday_map)\n\n# Set date as index of dataframe\n# data.set_index('date', inplace=True)\n\n\nSummary daily sales\n\n\n# Group by date and warehouse, sum sales, reset index right away\nds = (data.groupby([\"date\", \"warehouse\"], as_index=False).agg(sales=(\"sales\", \"sum\")).sort_values([\"warehouse\", \"date\"])\n)\n\nwarehouses = [\"Prague_1\", \"Prague_2\", \"Prague_3\", \"Brno_1\",\n              \"Budapest_1\", \"Frankfurt_1\", \"Munich_1\"]\n\n\nds[\"date\"] = pd.to_datetime(ds[\"date\"])\n\n# Plot each warehouse\nfor wh in warehouses:\n    df_wh = ds[ds[\"warehouse\"] == wh]\n    if df_wh.empty:\n        print(f\"⚠️ No data for {wh}\")\n        continue\n\n    plt.figure(figsize=(14, 6))\n    plt.plot(df_wh[\"date\"], df_wh[\"sales\"],\n             label=\"Daily Sales\", linewidth=1.0, color=\"purple\", alpha=0.7)\n    plt.title(f\"{wh} — Daily Sales\", fontsize=16, weight=\"bold\")\n    plt.xlabel(\"Date\", fontsize=12)\n    plt.ylabel(\"Sales\", fontsize=12)\n    plt.legend()\n    plt.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nds.head()\n\n\n\n\n\n\n\n\ndate\nwarehouse\nsales\n\n\n\n\n0\n2020-08-01\nBrno_1\n147396.27\n\n\n5\n2020-08-02\nBrno_1\n134427.58\n\n\n10\n2020-08-03\nBrno_1\n149112.67\n\n\n15\n2020-08-04\nBrno_1\n133365.47\n\n\n20\n2020-08-05\nBrno_1\n128492.28\n\n\n\n\n\n\n\nPrague and Brno are both in Czech Republic. Summary sales show simmilar repeated pattern of sales during every year with tendency to grow every year. Budapest is the capital of Hungary. Summary sales plot shows again repeated pattern of sales over years, but a bit different than plots from Czech Republic. There seems to be no tendency of growth over the years. City of Munich is in Germany. Plot of summary daily sales shows again repeated pattern of sales over every year with tendency to grow each year. City of Munich is in Germany. Plot of summary daily sales does not seems to show repeated pattern of sales over every year with tendency to grow each year.\n\n\nAverage sales during weekdays\n\nweek_average=data.groupby(['date', 'warehouse']).agg({'sales':'mean', 'day_of_week':'first', 'warehouse':'first'})\n\nfor wh in warehouses:\n        df_wh = week_average[week_average['warehouse'] == wh]\n\n        fig, ax = plt.subplots(figsize=(14, 6))\n        sns.barplot(x='day_of_week', y='sales', data=df_wh, ax=ax, palette='magma', order=('Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday','Saturday','Sunday'))\n\n        ax.set_title(f\"{wh} — Average Sales by Day of the Week\", fontsize=16, weight='bold')\n        ax.set_xlabel(\"Day of the Week\", fontsize=12)\n        ax.set_ylabel(\"Average Sales\", fontsize=12)\n        plt.tight_layout()\n        plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlots of average sales during weekdays shows tendency of custumers in all countries to do shopping on Thurdays and Fridays.\n\n\nAverage monthly sales\n\nmonthly_average=data.groupby(['date', 'warehouse']).agg({'sales':'mean', 'month':'first', 'warehouse':'first'})\n\nfor wh in warehouses:\n        df_wh = monthly_average[monthly_average['warehouse'] == wh]\n\n        fig, ax = plt.subplots(figsize=(14, 6))\n        sns.barplot(x='month', y='sales', data=df_wh, ax=ax, palette='magma', order=range(1,13))\n\n        ax.set_title(f\"{wh} — Average Sales by Month\", fontsize=16, weight='bold')\n        ax.set_xlabel(\"Month\", fontsize=12)\n        ax.set_ylabel(\"Average Sales\", fontsize=12)\n        plt.tight_layout()\n        plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlots of average monthly sales seems to reflect regionally differences in shopping habits of customers.\n\n\nAverage sales during holidays and common days\n\navg_holiday_sales=data.groupby(['date', 'warehouse']).agg({'sales':'mean', 'holiday':'first', 'holiday_name':'first', 'warehouse':'first'})\n\n\nfig, ax = plt.subplots(figsize=(14, 6))\nsns.barplot(x='warehouse', y='sales', hue='holiday', data=avg_holiday_sales, palette='magma')\n\nax.set_title(f\"{wh} — Average Sales: Holiday vs Non-Holiday\", fontsize=16, weight='bold')\nax.set_xlabel(\"Holiday\", fontsize=12)\nax.set_ylabel(\"Average Sales\", fontsize=12)\nplt.show()"
  },
  {
    "objectID": "notebooks/project.html#prediction",
    "href": "notebooks/project.html#prediction",
    "title": "Project",
    "section": "Prediction",
    "text": "Prediction\n\nData preparation\n\n# sort first\ndata = data.sort_values(['date', 'unique_id'])\n\n# lag features\nfor L in (1, 7, 14, 28):\n    data[f\"sales_lag_{L}\"] = data.groupby(\"unique_id\")[\"sales\"].shift(L)\n\n# rolling means/stds (past-only)\nfor W in (7, 14, 28):\n    data[f\"sales_roll{W}_mean\"] = (\n        data.groupby(\"unique_id\")[\"sales\"].shift(1).rolling(W).mean()\n    )\n    data[f\"sales_roll{W}_std\"] = (\n        data.groupby(\"unique_id\")[\"sales\"].shift(1).rolling(W).std()\n    )\n\n# simple momentum signals (optional)\ndata[\"sales_mom_1_7\"] = data[\"sales_lag_1\"] - data[\"sales_lag_7\"]\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 7340025 entries, 953925 to 4088300\nData columns (total 35 columns):\n #   Column                  Dtype         \n---  ------                  -----         \n 0   unique_id               int64         \n 1   date                    datetime64[ns]\n 2   warehouse               object        \n 3   total_orders            float64       \n 4   sales                   float64       \n 5   sell_price_main         float64       \n 6   availability            float64       \n 7   type_0_discount         float64       \n 8   type_1_discount         float64       \n 9   type_2_discount         float64       \n 10  type_3_discount         float64       \n 11  type_4_discount         float64       \n 12  type_5_discount         float64       \n 13  type_6_discount         float64       \n 14  holiday_name            object        \n 15  holiday                 object        \n 16  shops_closed            int64         \n 17  winter_school_holidays  int64         \n 18  school_holidays         int64         \n 19  year                    int64         \n 20  month                   int64         \n 21  day                     int64         \n 22  day_of_week             object        \n 23  weekend                 int32         \n 24  sales_lag_1             float64       \n 25  sales_lag_7             float64       \n 26  sales_lag_14            float64       \n 27  sales_lag_28            float64       \n 28  sales_roll7_mean        float64       \n 29  sales_roll7_std         float64       \n 30  sales_roll14_mean       float64       \n 31  sales_roll14_std        float64       \n 32  sales_roll28_mean       float64       \n 33  sales_roll28_std        float64       \n 34  sales_mom_1_7           float64       \ndtypes: datetime64[ns](1), float64(22), int32(1), int64(7), object(4)\nmemory usage: 1.9+ GB\n\n\n\ndata = data.dropna()\n\n\nH = 14\nframes = []\nfor h in range(1, H+1):\n    tmp = data.copy()\n    tmp[\"h\"] = h\n    tmp[\"target_h\"] = tmp.groupby(\"unique_id\")[\"sales\"].shift(-h)\n    frames.append(tmp)\n\nmh = pd.concat(frames, ignore_index=True)\nmh = mh.dropna(subset=[\"target_h\"]).reset_index(drop=True)\n\n\n\n# Using weights:\nwts = pd.read_csv(\"../rohlik_project/csv/test_weights.csv\")  # unique_id, weight\nmh = mh.merge(wts, on=\"unique_id\", how=\"left\").fillna({\"weight\": 1.0})\n\ndef wmape(y_true, y_pred, w=None, eps=1e-9):\n    if w is None:\n        return np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + eps)\n    return np.sum(w * np.abs(y_true - y_pred)) / (np.sum(w * np.abs(y_true)) + eps)\n\n\nmh.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 4204647 entries, 0 to 4204646\nData columns (total 38 columns):\n #   Column                  Dtype         \n---  ------                  -----         \n 0   unique_id               int64         \n 1   date                    datetime64[ns]\n 2   warehouse               object        \n 3   total_orders            float64       \n 4   sales                   float64       \n 5   sell_price_main         float64       \n 6   availability            float64       \n 7   type_0_discount         float64       \n 8   type_1_discount         float64       \n 9   type_2_discount         float64       \n 10  type_3_discount         float64       \n 11  type_4_discount         float64       \n 12  type_5_discount         float64       \n 13  type_6_discount         float64       \n 14  holiday_name            object        \n 15  holiday                 object        \n 16  shops_closed            int64         \n 17  winter_school_holidays  int64         \n 18  school_holidays         int64         \n 19  year                    int64         \n 20  month                   int64         \n 21  day                     int64         \n 22  day_of_week             object        \n 23  weekend                 int32         \n 24  sales_lag_1             float64       \n 25  sales_lag_7             float64       \n 26  sales_lag_14            float64       \n 27  sales_lag_28            float64       \n 28  sales_roll7_mean        float64       \n 29  sales_roll7_std         float64       \n 30  sales_roll14_mean       float64       \n 31  sales_roll14_std        float64       \n 32  sales_roll28_mean       float64       \n 33  sales_roll28_std        float64       \n 34  sales_mom_1_7           float64       \n 35  h                       int64         \n 36  target_h                float64       \n 37  weight                  float64       \ndtypes: datetime64[ns](1), float64(24), int32(1), int64(8), object(4)\nmemory usage: 1.2+ GB\n\n\n\nmh = mh.copy()\n\n# Columns to label-encode\ncat_cols = ['warehouse', 'day_of_week', 'holiday', 'holiday_name']\n\n# Apply LabelEncoder to each column\nfor col in cat_cols:\n    le = LabelEncoder()\n    mh[col] = le.fit_transform(mh[col].astype(str))  # convert to string for safety\n\n\n\nModel and prediction\n\n# feature columns: keep engineered stuff + calendar flags; drop IDs/date/targets\ndrop_cols = {\"sales\",\"target_h\",\"date\"}  # drop raw sales & date\nid_cols = {\"unique_id\",\"warehouse\"}\nmaybe_features = [\n    c for c in mh.columns\n    if c not in drop_cols | id_cols and not c.endswith(\"_name\")\n]\n\n# keep typical useful features\nfeatures = [\n    c for c in maybe_features\n    if c.startswith(\"sales_\") or c in {\"holiday\",\"shops_closed\",\"school_holidays\",\"winter_school_holidays\",\n                                       \"day_of_week\",\"month\",\"year\",\"h\",\"total_orders\",\"sell_price_main\",\n                                       \"availability\", \"Sales_lag_1\", \"Sales_lag_7\", \"Sales_lag_14\", \"Sales_lag_28\",\n                                       \"rolling_mean7\", \"rolling_std7\", \"rolling_mean14\", \"rolling_std14\",\n                                       \"rolling_mean28\", \"rolling_std28\", \"sales_mom_1_7\"} \n]\n\nX = mh[features].copy()\ny = mh[\"target_h\"].astype(float).values\nw = mh[\"weight\"].values if \"weight\" in mh else None\n\n# simple validation: last 14 days across all series\ncutoff = mh[\"date\"].max() - pd.Timedelta(days=H)\ntrain_idx = mh[\"date\"] &lt; cutoff\nvalid_idx = mh[\"date\"] &gt;= cutoff\n\nX_tr, y_tr = X[train_idx], y[train_idx]\nX_va, y_va = X[valid_idx], y[valid_idx]\nw_tr = w[train_idx] if w is not None else None\nw_va = w[valid_idx] if w is not None else None\n\n\nmodel = lgb.LGBMRegressor(\n    objective=\"tweedie\",\n    tweedie_variance_power=1.4,  \n    n_estimators=1500,\n    learning_rate=0.05,\n    num_leaves=63,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=1337\n)\n\n# Train safely across all LightGBM versions\nmodel.fit(\n    X_tr, y_tr,\n    sample_weight=w_tr,                 \n    eval_set=[(X_va, y_va)],\n    eval_metric=\"mae\",\n    callbacks=[lgb.log_evaluation(period=200)]  \n)\n\npred_va = np.clip(model.predict(X_va), 0, None)  # non-negative\nprint(\"MAE  :\", mean_absolute_error(y_va, pred_va))\nprint(\"WMAPE:\", wmape(y_va, pred_va, w=w_va))\n\n\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237982 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3463\n[LightGBM] [Info] Number of data points in the train set: 4177122, number of used features: 22\n[LightGBM] [Info] Start training from score 4.271974\n[200]   valid_0's l1: 33.6998   valid_0's tweedie: 59.8174\n[400]   valid_0's l1: 33.1148   valid_0's tweedie: 59.7952\n[600]   valid_0's l1: 33.4849   valid_0's tweedie: 59.7889\n[800]   valid_0's l1: 33.3251   valid_0's tweedie: 59.7844\n[1000]  valid_0's l1: 33.0742   valid_0's tweedie: 59.7811\n[1200]  valid_0's l1: 33.1621   valid_0's tweedie: 59.7765\n[1400]  valid_0's l1: 33.138    valid_0's tweedie: 59.7751\nMAE  : 33.10275893556487\nWMAPE: 0.3124203275922004\n\n\n\ngrid = mh.copy()\n\n\n# weekday averages with sensible fallbacks → solution.csv \n\n# copies & types\ntrain = sales_train_df.copy()\ntest  = sales_test_df.copy()\ntrain[\"date\"] = pd.to_datetime(train[\"date\"])\ntest[\"date\"]  = pd.to_datetime(test[\"date\"])\n\n# day of week (0=Mon..6=Sun)\ntrain[\"dow\"] = train[\"date\"].dt.dayofweek\ntest[\"dow\"]  = test[\"date\"].dt.dayofweek\n\nkeys = [\"warehouse\",\"unique_id\"]\n\n# 1) Core stats\n# per series & weekday\nmean_ser_dow = (train.groupby(keys + [\"dow\"])[\"sales\"].mean()\n                     .rename(\"mean_ser_dow\").reset_index())\n# per series overall\nmean_ser = (train.groupby(keys)[\"sales\"].mean()\n                 .rename(\"mean_ser\").reset_index())\n# per warehouse & weekday\nmean_wh_dow = (train.groupby([\"warehouse\",\"dow\"])[\"sales\"].mean()\n                    .rename(\"mean_wh_dow\").reset_index())\n# global weekday\nmean_glob_dow = (train.groupby(\"dow\")[\"sales\"].mean()\n                      .rename(\"mean_glob_dow\").reset_index())\n# global overall\nmean_glob = float(train[\"sales\"].mean())\n\n# 2) Merge in descending strength and build fallback\npred = test.copy()\npred = pred.merge(mean_ser_dow, on=keys+[\"dow\"], how=\"left\")\npred = pred.merge(mean_ser,     on=keys,         how=\"left\")\npred = pred.merge(mean_wh_dow,  on=[\"warehouse\",\"dow\"], how=\"left\")\npred = pred.merge(mean_glob_dow,on=[\"dow\"],      how=\"left\")\n\n# fallback chain: series&dow → series → wh&dow → global&dow → global\npred_vals = (\n    pred[\"mean_ser_dow\"]\n    .fillna(pred[\"mean_ser\"])\n    .fillna(pred[\"mean_wh_dow\"])\n    .fillna(pred[\"mean_glob_dow\"])\n    .fillna(mean_glob)\n)\n\n# 3) Non-negative and reasonable caps\npred_vals = np.clip(pred_vals.values, 0, None)\n\n# 4) Build submission\nsolution = test[[\"warehouse\",\"unique_id\",\"date\"]].copy()\nsolution[\"id\"] = (\n    solution[\"warehouse\"].astype(str) + \"_\" +\n    solution[\"unique_id\"].astype(str) + \"_\" +\n    solution[\"date\"].dt.strftime(\"%Y-%m-%d\")\n)\nsolution[\"sales_hat\"] = pred_vals.astype(float)\nsolution = solution[[\"id\",\"sales_hat\"]]\n\n# Checks & save\nassert len(solution) == len(test), \"Row count mismatch vs sales_test_df.\"\nassert solution[\"sales_hat\"].notna().all(), \"Found NaNs in predictions.\"\n\nsolution.to_csv(\"csv/solution.csv\", index=False)\nprint(\" Saved solution.csv with\", len(solution), \"rows.\")\n\n Saved solution.csv with 47021 rows.\n\n\n\n\n\nEnd"
  },
  {
    "objectID": "notebooks/penguins.html",
    "href": "notebooks/penguins.html",
    "title": "Penguins",
    "section": "",
    "text": "This notebook is my attempt to analyze penguins dataset.\n\n# Libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nimport scipy.stats\n\n\nLoading\n\n#Loading the Penguins dataset.\ndf = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\")\n\n\ndf.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n\n\n\n\n\n\n# Information about dataframe\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\n\n\nCleaning dataset\n\n# Checking missing values\ndf.isnull().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\n\n# Removing missing values\ndf.dropna(axis=0, inplace=True)\n\n\ndf.isnull().sum()\n\nspecies              0\nisland               0\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\n\n# Checking multiple values\ndf.duplicated()\n\n0      False\n1      False\n2      False\n4      False\n5      False\n       ...  \n338    False\n340    False\n341    False\n342    False\n343    False\nLength: 333, dtype: bool\n\n\nNo duplicate values. Now, I will work with dateset with only 333 rows.\n\n# Correcting index\ndf.reset_index(drop=True)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMALE\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n328\nGentoo\nBiscoe\n47.2\n13.7\n214.0\n4925.0\nFEMALE\n\n\n329\nGentoo\nBiscoe\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n\n\n330\nGentoo\nBiscoe\n50.4\n15.7\n222.0\n5750.0\nMALE\n\n\n331\nGentoo\nBiscoe\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n\n\n332\nGentoo\nBiscoe\n49.9\n16.1\n213.0\n5400.0\nMALE\n\n\n\n\n333 rows × 7 columns\n\n\n\n\n\nDescribing dataset\n\ncount1=df['species'].value_counts()\ncount2=df['island'].value_counts()\ncount3=df['sex'].value_counts()\nprint(count1,count2,count3)\n\nspecies\nAdelie       146\nGentoo       119\nChinstrap     68\nName: count, dtype: int64 island\nBiscoe       163\nDream        123\nTorgersen     47\nName: count, dtype: int64 sex\nMALE      168\nFEMALE    165\nName: count, dtype: int64\n\n\n\nwith sns.color_palette('deep'):\n    sns.barplot(data=df, x='island', y=df.index, hue='species')\n\n\n\n\n\n\n\n\n\nwith sns.color_palette('pastel'):\n    sns.barplot(data=df, x='species', y=df.species.index, hue='sex')\n\n\n\n\n\n\n\n\nThis dataset is not balanced, containing various counts of values for variables as it came from field ecological research.\n\nHistograms of morphological values\nA histogram is a visualization tool that represents the distribution of one or more variables by counting the number of observations which fall within discrete bins. Parameter kde=, if True, compute a kernel density estimate to smooth the distribution and show on the plot as (one or more) line(s). (https://seaborn.pydata.org/generated/seaborn.histplot.html)\n\nwith sns.color_palette('bright6'):\n    sns.histplot(data=df, x='body_mass_g', hue='species', element='step', kde=True, bins=25, )\n\n\n\n\n\n\n\n\nGentoo penguins are heavier than Adelie penguins and Chinstrap penguins.\n\nwith sns.color_palette('bright6'):\n    sns.histplot(data=df, x='bill_length_mm', hue='species', element='step', kde=True, bins=25)\n\n\n\n\n\n\n\n\nAdelie penguins have shorter bills than Chinstrap penguins and Gentoo penguins.\n\nwith sns.color_palette('bright6'):\n    sns.histplot(data=df, x='bill_depth_mm', hue='species', element='step', kde=True, bins=25)\n\n\n\n\n\n\n\n\nBill depth is lower with Gentoo penguins than with Adelie penguins and Chinstrap penguins.\n\nwith sns.color_palette('bright6'):\n    sns.histplot(data=df, x='flipper_length_mm', hue='species', element='step',kde=True, bins=25)\n\n\n\n\n\n\n\n\nGentoo penguins have longer flipper than Adelie penguins and Chinstrap penguins.\n\n\n\nCorrelation\nCorrelation or dependence, is a relationship between variables. It could be positive - with one variable increasing, the other one increasing, too - or negative - with one variable increasing, the other one decreasing. I am going to make a comparison of flipper_length_mm and body_mass_g. The flipper is a big limb on the penguin’s body. I assume a positive correlation between the length of the flipper and the weight of the body of the whole penguin. This can be proved by the model of linear regression (https://realpython.com/linear-regression-in-python/#simple-linear-regression).\n\nLinear Regression Model\n\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nx = df['flipper_length_mm']\ny = df['body_mass_g']\n\nslope, intercept, r, p, std_err = stats.linregress(x, y)\n\ndef reg(x):\n  return slope * x + intercept\n\nmodel = list(map(reg, x))\n\nplt.scatter(x,y)\nplt.plot(x, model)\nplt.xlabel('flipper_length_mm')\nplt.ylabel('body_mass_g')\nplt.show()\n\n\n\n\n\n\n\n\nSolid, positive correlation as fit line and distribution of values are oriented to the diagonal of the graph.\nCounting of correlation coefficient’s values for whole dataset can show another possible relationships between values.\n\n\nEncoding\nChange of datatypes into int64 can help me to count the correlation of all variables. (https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python/)\n\n# Encoding\nencode=LabelEncoder()\ndf.species=encode.fit_transform(df.species)\ndf.island=encode.fit_transform(df.island)\ndf.sex=encode.fit_transform(df.sex)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 333 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            333 non-null    int32  \n 1   island             333 non-null    int32  \n 2   bill_length_mm     333 non-null    float64\n 3   bill_depth_mm      333 non-null    float64\n 4   flipper_length_mm  333 non-null    float64\n 5   body_mass_g        333 non-null    float64\n 6   sex                333 non-null    int32  \ndtypes: float64(4), int32(3)\nmemory usage: 16.9 KB\n\n\n\ndf.astype('int64').dtypes\n\nspecies              int64\nisland               int64\nbill_length_mm       int64\nbill_depth_mm        int64\nflipper_length_mm    int64\nbody_mass_g          int64\nsex                  int64\ndtype: object\n\n\nAdelie has code 0, Chinstrap 1, Gentoo 2, male 0, female 1, island Torgensen 2, Dream island 1, Biscoe 0. The datatype was changed from object to integer. This unification can help me to analyze the whole dataset as some methods of statistics work with numbers mainly. Optimal datatype for model variables in this dataset is a number - integer64 as morphological values measured in mm and grams will stay precise.\n\n\nCorrelation Matrix\nThe default value of the parameter method of corr() function is ‘pearson’ - which means Pearson correlation coefficient. (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).\n\ncorr_coef=df.corr(method='pearson')\nsns.heatmap(corr_coef, annot=True, cmap='coolwarm', vmin=-1,vmax=1)\n\n\n\n\n\n\n\n\nResults between -0.59 and 0.59 indicate an omissible correlation. Variable island and sex shows only a weak correlation. Some correlation is between flipper_lenght_mm and bill_lenght_mm and between flipper_lenght_mm and bodymass_g. Species is a variable with correlation to any other variable which is not a surprise. Surprise is a weak correlation between sex and morphological values which is a misrepresentation given by containing values for males and females of three species in the same categories.\n\n\n\n\nEnd\n\n\n\nAdelie"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lucia Macakova",
    "section": "",
    "text": "Data Analysis\n\nPrediction of sales - Rohlik project\nAnalysis of classical Iris dataset - Iris analysis\nAnalyses of penguins dataset - Penguins\nA bit of theory of statistics - Applied statistics – problems\n\nInventory Management App - Simple web-based inventory management application built with Python\nluciamacakova.pythonanywhere.com\nAutomated data colection pipeline - weather_data_pipeline - Automated data colection pipeline"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "Lucia Macakova",
    "section": "",
    "text": "A Student of programme Higher Diploma in Science in Computing in Data Analytics\nAtlantic TEchnological University - Campus Galway\n\nEmail: G00439449@atu.ie"
  },
  {
    "objectID": "notebooks/iris_analysis.html",
    "href": "notebooks/iris_analysis.html",
    "title": "Data Analysis of Iris Dataset",
    "section": "",
    "text": "iris species\n# Module imports\nimport sys, platform, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\nimport warnings, random\nwarnings.filterwarnings(\"ignore\")\nimport os\n\nnp.random.seed(42)\nrandom.seed(42)\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy import stats\nfrom scipy.stats import f_oneway, kruskal, shapiro, levene\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nimport ipywidgets as widgets"
  },
  {
    "objectID": "notebooks/iris_analysis.html#loading-data",
    "href": "notebooks/iris_analysis.html#loading-data",
    "title": "Data Analysis of Iris Dataset",
    "section": "Loading data",
    "text": "Loading data\n\ndf = pd.read_csv(\"iris/iris.csv\", header=0)\ndisplay(df.head())\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa"
  },
  {
    "objectID": "notebooks/iris_analysis.html#description-of-dataset",
    "href": "notebooks/iris_analysis.html#description-of-dataset",
    "title": "Data Analysis of Iris Dataset",
    "section": "Description of dataset",
    "text": "Description of dataset\n\ndisplay(df.info())\ndisplay(df.describe().T)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n\n\nNone\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nsepal_length\n150.0\n5.843333\n0.828066\n4.3\n5.1\n5.80\n6.4\n7.9\n\n\nsepal_width\n150.0\n3.054000\n0.433594\n2.0\n2.8\n3.00\n3.3\n4.4\n\n\npetal_length\n150.0\n3.758667\n1.764420\n1.0\n1.6\n4.35\n5.1\n6.9\n\n\npetal_width\n150.0\n1.198667\n0.763161\n0.1\n0.3\n1.30\n1.8\n2.5\n\n\n\n\n\n\n\n\nChecking data quality\n\nprint(\"Missing values per column:\\n\", df.isna().sum())\nprint(\"Duplicate rows:\", df.duplicated().sum())\n\nMissing values per column:\n sepal_length    0\nsepal_width     0\npetal_length    0\npetal_width     0\nspecies         0\ndtype: int64\nDuplicate rows: 3\n\n\nI have not found any missing values. Duplicates reflect real repeated values in the source.\n\n\nSummaries\n\nfeatures = [\"sepal_length\",\"sepal_width\",\n            \"petal_length\",\"petal_width\"]\nfor feat in features:\n    desc = df[feat].describe().T\n    display(desc)\n    desc.to_csv(os.path.join(\"summaries\", f\"{feat}.csv\"))\n\ncount    150.000000\nmean       5.843333\nstd        0.828066\nmin        4.300000\n25%        5.100000\n50%        5.800000\n75%        6.400000\nmax        7.900000\nName: sepal_length, dtype: float64\n\n\ncount    150.000000\nmean       3.054000\nstd        0.433594\nmin        2.000000\n25%        2.800000\n50%        3.000000\n75%        3.300000\nmax        4.400000\nName: sepal_width, dtype: float64\n\n\ncount    150.000000\nmean       3.758667\nstd        1.764420\nmin        1.000000\n25%        1.600000\n50%        4.350000\n75%        5.100000\nmax        6.900000\nName: petal_length, dtype: float64\n\n\ncount    150.000000\nmean       1.198667\nstd        0.763161\nmin        0.100000\n25%        0.300000\n50%        1.300000\n75%        1.800000\nmax        2.500000\nName: petal_width, dtype: float64\n\n\nI saved summaries for sepal length, sepal width, petal length, petal width into subfolder summaries.\n\n# Loop through species\nfor sp, group in df.groupby(\"species\"):\n    print(f\"\\nSummary for species: {sp}\\n\")   # &lt;-- species name shown\n    desc = group.describe().T                 # transpose for nicer layout\n    display(desc)\n    desc.to_csv(os.path.join(\"summaries\", f\"{sp}_summary.csv\"))\n\n\nSummary for species: Iris-setosa\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nsepal_length\n50.0\n5.006\n0.352490\n4.3\n4.800\n5.0\n5.200\n5.8\n\n\nsepal_width\n50.0\n3.418\n0.381024\n2.3\n3.125\n3.4\n3.675\n4.4\n\n\npetal_length\n50.0\n1.464\n0.173511\n1.0\n1.400\n1.5\n1.575\n1.9\n\n\npetal_width\n50.0\n0.244\n0.107210\n0.1\n0.200\n0.2\n0.300\n0.6\n\n\n\n\n\n\n\n\nSummary for species: Iris-versicolor\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nsepal_length\n50.0\n5.936\n0.516171\n4.9\n5.600\n5.90\n6.3\n7.0\n\n\nsepal_width\n50.0\n2.770\n0.313798\n2.0\n2.525\n2.80\n3.0\n3.4\n\n\npetal_length\n50.0\n4.260\n0.469911\n3.0\n4.000\n4.35\n4.6\n5.1\n\n\npetal_width\n50.0\n1.326\n0.197753\n1.0\n1.200\n1.30\n1.5\n1.8\n\n\n\n\n\n\n\n\nSummary for species: Iris-virginica\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nsepal_length\n50.0\n6.588\n0.635880\n4.9\n6.225\n6.50\n6.900\n7.9\n\n\nsepal_width\n50.0\n2.974\n0.322497\n2.2\n2.800\n3.00\n3.175\n3.8\n\n\npetal_length\n50.0\n5.552\n0.551895\n4.5\n5.100\n5.55\n5.875\n6.9\n\n\npetal_width\n50.0\n2.026\n0.274650\n1.4\n1.800\n2.00\n2.300\n2.5\n\n\n\n\n\n\n\nI saved summaries for entire species into folder summaries"
  },
  {
    "objectID": "notebooks/iris_analysis.html#explanatory-data-analysis",
    "href": "notebooks/iris_analysis.html#explanatory-data-analysis",
    "title": "Data Analysis of Iris Dataset",
    "section": "Explanatory data analysis",
    "text": "Explanatory data analysis\n\nHistograms of morphological values\nHistogram is the plot that shows distribution of values. I decided to add lines of kernel density estimation that smooths data distribution for each species of Iris for better data representation.\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n\nfeatures = [\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]\n\nfor feat in features:\n    plt.figure()\n    sns.histplot(df, x=feat, hue=\"species\", bins=25, kde=True,\n                 element=\"step\", fill=True, palette=\"magma\", alpha=0.7)\n    plt.xlabel(feat.replace(\"_\",\" \").title() + \" (cm)\")\n    plt.savefig(os.path.join(\"plots\", f\"{feat}_hist.png\"))\n    plt.show()\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to histograms, flowers of Iris Virginica and Iris Versicolor share similar values, with mean value for Iris Virginica’s flowers higher in any category than for flowers of Iris Versicolor. Flowers of Iris Setosa differ from these species with lower values for petal length, petal width, sepal length, but higher values for sepal width than for Iris Virginica and Iris Versicolor. All histograms are saved in the folder histograms.\n\n\nViolin plot\nViolin plot combines the information of boxplot and density plot. The “violin” shows the full distribution of a feature for each Iris species, while also marking the median and quartiles. https://seaborn.pydata.org/generated/seaborn.violinplot.html\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\nfor ax, feat in zip(axes.flat, features):\n    sns.violinplot(\n        data=df, x=\"species\", y=feat,\n        palette=\"magma\", ax=ax\n    )\n    ax.set_title(f\"Distribution of {feat.replace('_',' ').title()}\")\n    ax.set_xlabel(\"Species\")\n    ax.set_ylabel(\"Value (cm)\")\n\nplt.tight_layout()\nplt.savefig(os.path.join('plots', \"violin_plots.png\"))\nplt.show()\n\n\n\n\n\n\n\n\nFor petal length and width, Setosa is completely separated from the other two species, which is consistent with histograms.\nVersicolor and Virginica overlap, though Virginica generally has larger values.\nFor sepal length and width, all three species overlap more, showing these features are less useful for classification.\nOverall, violin plots confirm that petal features carry the strongest discriminatory power.\n\n\nCorrelation matrix\n\n# Correlation heatmap\noutput_plot = \"other_plots\"\nplt.figure(figsize=(6,5))\nsns.heatmap(df.corr(numeric_only=True), annot=True, vmin=-1, vmax=1, square=True)\nplt.title(\"Pearson correlation\")\nplt.tight_layout()\nplt.savefig(os.path.join('plots', 'correlation_heatmap.png'))\nplt.show()\nimport voila\n\n\n\n\n\n\n\n\nValues above 0.59 can be considered as “solid” correlation. Petal length is strongly correlated with petal width along with sepal length, while sepal width is not strongly correlated with any other variable.\nhttps://seaborn.pydata.org/generated/seaborn.heatmap.html\n\n\nScatterplots\n\n# Pairplot with species hue\nsns.pairplot(df, hue=\"species\", corner=True, diag_kind='kde', palette=\"magma\")\nplt.savefig(os.path.join('plots', 'pairplot.png'))\nplt.show()\n\n\n\n\n\n\n\n\nThe pairplot shows the relationship between all pairs of features, with colors for each Iris species.\nPetal length vs. petal width: the strong positive correlation seen in the correlation matrix (≈0.96) is clearly visible as an almost straight line of points.\nSepal length has moderate positive correlation with petal features, visible as slanted clouds.\nSepal width shows weaker or even negative correlations, confirmed by the scattered patterns.\nSetosa is perfectly separated from the other species in petal features, while Versicolor and Virginica overlap.\nThis confirms the numerical correlation results and visually highlights which features best discriminate the species.\nhttps://seaborn.pydata.org/generated/seaborn.scatterplot.html"
  },
  {
    "objectID": "notebooks/iris_analysis.html#classification",
    "href": "notebooks/iris_analysis.html#classification",
    "title": "Data Analysis of Iris Dataset",
    "section": "Classification",
    "text": "Classification\n\n# Normality test (Shapiro-Wilk)\nfor feat in features:\n    print(f\"\\nFeature: {feat}\")\n    for sp, group in df.groupby(\"species\"):\n        stat, p = shapiro(group[feat])\n        print(f\"  {sp:12s} | W={stat:.3f}, p={p:.5f} \"\n              f\"| {'Normal ✅' if p &gt; 0.05 else 'Not normal ❌'}\")\n\n\nFeature: sepal_length\n  Iris-setosa  | W=0.978, p=0.45950 | Normal ✅\n  Iris-versicolor | W=0.978, p=0.46474 | Normal ✅\n  Iris-virginica | W=0.971, p=0.25831 | Normal ✅\n\nFeature: sepal_width\n  Iris-setosa  | W=0.969, p=0.20465 | Normal ✅\n  Iris-versicolor | W=0.974, p=0.33798 | Normal ✅\n  Iris-virginica | W=0.967, p=0.18090 | Normal ✅\n\nFeature: petal_length\n  Iris-setosa  | W=0.955, p=0.05465 | Normal ✅\n  Iris-versicolor | W=0.966, p=0.15848 | Normal ✅\n  Iris-virginica | W=0.962, p=0.10978 | Normal ✅\n\nFeature: petal_width\n  Iris-setosa  | W=0.814, p=0.00000 | Not normal ❌\n  Iris-versicolor | W=0.948, p=0.02728 | Not normal ❌\n  Iris-virginica | W=0.960, p=0.08695 | Normal ✅\n\n\nI tested normality of distribution of features within each species with Shapiro=Wilk test.\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html\n\n# Search for outliers using IQR method\nfor feat in features:\n    Q1 = df[feat].quantile(0.25)\n    Q3 = df[feat].quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = df[(df[feat] &lt; Q1 - 1.5*IQR) | (df[feat] &gt; Q3 + 1.5*IQR)]\n    print(f\"{feat}: {len(outliers)} outliers\")\n\nsepal_length: 0 outliers\nsepal_width: 4 outliers\npetal_length: 0 outliers\npetal_width: 0 outliers\n\n\nI found some outliers.\n\n# Homogeneity of variances (Levene's test)\nfor col in features:\n    groups = [g[col].values for _, g in df.groupby(\"species\")]\n    stat, p = levene(*groups)\n    print(f\"{col}: Levene stat={stat:.2f}, p={p:.5f}\")\n\nsepal_length: Levene stat=6.35, p=0.00226\nsepal_width: Levene stat=0.65, p=0.52483\npetal_length: Levene stat=19.72, p=0.00000\npetal_width: Levene stat=19.41, p=0.00000\n\n\nThe Levene test tests the null hypothesis that all input samples are from populations with equal variances.Only sepal width is different than other features, when variances are compared\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.levene.html\n\n#ANOVA across species for each feature\nfor col in features:\n    groups = [g[col].values for _, g in df.groupby(\"species\")]\n    f, p = stats.f_oneway(*groups)\n    print(f\"{col}: ANOVA F={f:.2f}, p={p:.10f}\")\n\n# If any feature looks non-normal: use nonparametric alternative\nfor col in features:\n    groups = [g[col].values for _, g in df.groupby(\"species\")]\n    h, p = stats.kruskal(*groups)\n    print(f\"{col}: Kruskal H={h:.2f}, p={p:.10f}\")\n\nsepal_length: ANOVA F=119.26, p=0.0000000000\nsepal_width: ANOVA F=47.36, p=0.0000000000\npetal_length: ANOVA F=1179.03, p=0.0000000000\npetal_width: ANOVA F=959.32, p=0.0000000000\nsepal_length: Kruskal H=96.94, p=0.0000000000\nsepal_width: Kruskal H=62.49, p=0.0000000000\npetal_length: Kruskal H=130.41, p=0.0000000000\npetal_width: Kruskal H=131.09, p=0.0000000000\n\n\nBefore running ANOVA, I checked for outliers and normality. A few extreme values were found in sepal width, but they are consistent with real biological variation and not obvious data entry errors. Since ANOVA is sensitive to outliers and non-normal distribution of data, I also verified the results with the Kruskal–Wallis test, which is more robust to non-normality and extreme values. Both tests agreed, so I kept the outliers in the analysis. ANOVA and Kruskal–Wallis tests produced very small p-values (&lt; 0.00001), indicating strong evidence against the null hypothesis of equal group means. This confirms that at least one species differs significantly from the others.\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html\n\nfor feat in features:\n    print(f\"\\nTukey HSD results for {feat}:\\n\")\n    tukey = pairwise_tukeyhsd(df[feat], df[\"species\"], alpha=0.05)\n    print(tukey.summary())\n\n\nTukey HSD results for sepal_length:\n\n        Multiple Comparison of Means - Tukey HSD, FWER=0.05        \n===================================================================\n     group1          group2     meandiff p-adj lower  upper  reject\n-------------------------------------------------------------------\n    Iris-setosa Iris-versicolor     0.93   0.0 0.6862 1.1738   True\n    Iris-setosa  Iris-virginica    1.582   0.0 1.3382 1.8258   True\nIris-versicolor  Iris-virginica    0.652   0.0 0.4082 0.8958   True\n-------------------------------------------------------------------\n\nTukey HSD results for sepal_width:\n\n         Multiple Comparison of Means - Tukey HSD, FWER=0.05         \n=====================================================================\n     group1          group2     meandiff p-adj  lower   upper  reject\n---------------------------------------------------------------------\n    Iris-setosa Iris-versicolor   -0.648   0.0 -0.8092 -0.4868   True\n    Iris-setosa  Iris-virginica   -0.444   0.0 -0.6052 -0.2828   True\nIris-versicolor  Iris-virginica    0.204 0.009  0.0428  0.3652   True\n---------------------------------------------------------------------\n\nTukey HSD results for petal_length:\n\n        Multiple Comparison of Means - Tukey HSD, FWER=0.05        \n===================================================================\n     group1          group2     meandiff p-adj lower  upper  reject\n-------------------------------------------------------------------\n    Iris-setosa Iris-versicolor    2.796   0.0 2.5922 2.9998   True\n    Iris-setosa  Iris-virginica    4.088   0.0 3.8842 4.2918   True\nIris-versicolor  Iris-virginica    1.292   0.0 1.0882 1.4958   True\n-------------------------------------------------------------------\n\nTukey HSD results for petal_width:\n\n        Multiple Comparison of Means - Tukey HSD, FWER=0.05        \n===================================================================\n     group1          group2     meandiff p-adj lower  upper  reject\n-------------------------------------------------------------------\n    Iris-setosa Iris-versicolor    1.082   0.0 0.9849 1.1791   True\n    Iris-setosa  Iris-virginica    1.782   0.0 1.6849 1.8791   True\nIris-versicolor  Iris-virginica      0.7   0.0 0.6029 0.7971   True\n-------------------------------------------------------------------\n\n\nPost-hoc Tukey test confirmed that the species are different from each other in every feature.\n\n# Data preparation\nX = df.drop(columns=\"species\").values\ny = df[\"species\"].values\n\n# scaler=StandardScaler()\n# X_scaled=scaler.fit_transform(X)\n\nle = LabelEncoder()\ny_enc = le.fit_transform(y)\n\n\n\n# LDA 2D projection (max 2 components for 3 classes)\nlda=make_pipeline( StandardScaler(), LinearDiscriminantAnalysis(n_components=2))\nX_lda = lda.fit_transform(X, y)\nplt.figure()\ncolors = sns.color_palette(\"magma\", 3)\nfor i, sp in enumerate(le.classes_):\n    plt.scatter(X_lda[y_enc==i,0], X_lda[y_enc==i,1], label=sp, s=35, alpha=0.7, color=colors[i])\nplt.legend();\nplt.title(\"LDA (2D)\");\nplt.xlabel(\"LD1\");\nplt.ylabel(\"LD2\");\nplt.savefig(os.path.join('plots', 'lda_2d_projection.png'))\nplt.show()\n\n\n\n\n\n\n\n\nLDA reduces the 4D Iris dataset to 2D by finding axes that maximize species separation. The scatterplot of LD1 vs. LD2 shows that Setosa is perfectly separated, while Versicolor and Virginica overlap partially. This confirms that petal features dominate separation, and LDA effectively summarizes class structure.\nhttps://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html\n\n# Train/test split (stratified)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\nmodels = {\n    \"LogisticRegression\": make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000)),\n    \"LDA\": make_pipeline(StandardScaler(), LinearDiscriminantAnalysis()),\n    \"DecisionTree\": make_pipeline(DecisionTreeClassifier(random_state=42))\n}\n\n# Helper: CV accuracy\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ndef cv_acc(name, est):\n    scores = cross_val_score(est, X, y, cv=cv, scoring=\"accuracy\")\n    print(f\"{name:&gt;16} | CV Acc: {scores.mean():.3f} ± {scores.std():.3f}\")\n\nprint(\"Cross-validated accuracy:\")\nfor name, est in models.items():\n    cv_acc(name, est)\n\n# Final train on train set, evaluate on held-out test set\nprint(\"\\nHeld-out test performance:\")\nfor name, est in models.items():\n    est.fit(X_train, y_train)\n    y_pred = est.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    print(f\"{name:&gt;16} | Test Acc: {acc:.3f}\")\n    print(classification_report(y_test, y_pred, digits=3))\n    \n\nCross-validated accuracy:\nLogisticRegression | CV Acc: 0.953 ± 0.045\n             LDA | CV Acc: 0.973 ± 0.039\n    DecisionTree | CV Acc: 0.953 ± 0.034\n\nHeld-out test performance:\nLogisticRegression | Test Acc: 0.921\n                 precision    recall  f1-score   support\n\n    Iris-setosa      1.000     1.000     1.000        12\nIris-versicolor      0.857     0.923     0.889        13\n Iris-virginica      0.917     0.846     0.880        13\n\n       accuracy                          0.921        38\n      macro avg      0.925     0.923     0.923        38\n   weighted avg      0.923     0.921     0.921        38\n\n             LDA | Test Acc: 1.000\n                 precision    recall  f1-score   support\n\n    Iris-setosa      1.000     1.000     1.000        12\nIris-versicolor      1.000     1.000     1.000        13\n Iris-virginica      1.000     1.000     1.000        13\n\n       accuracy                          1.000        38\n      macro avg      1.000     1.000     1.000        38\n   weighted avg      1.000     1.000     1.000        38\n\n    DecisionTree | Test Acc: 0.895\n                 precision    recall  f1-score   support\n\n    Iris-setosa      1.000     1.000     1.000        12\nIris-versicolor      0.800     0.923     0.857        13\n Iris-virginica      0.909     0.769     0.833        13\n\n       accuracy                          0.895        38\n      macro avg      0.903     0.897     0.897        38\n   weighted avg      0.900     0.895     0.894        38\n\n\n\nI compared three classification models on the Iris dataset: Logistic Regression, Linear Discriminant Analysis (LDA), and a Decision Tree.\nTo estimate general performance, I used 5-fold stratified cross-validation. The mean and standard deviation of accuracy across folds show the stability of each model.\nI trained each model on the training set (75% of data) and evaluated it on a held-out test set (25%).\nFor each model, I reported:\n- Overall test accuracy. - A detailed classification report (precision, recall, F1-score). - A confusion matrix to visualize misclassifications.\nThe best accuracy was reached with LDA.\n\n\nEnd"
  },
  {
    "objectID": "notebooks/problems.html",
    "href": "notebooks/problems.html",
    "title": "Problems",
    "section": "",
    "text": "import sys\n# https://docs.python.org/3/library/sys.html - system-specific parameters and functions\nimport math\n# https://docs.python.org/3/library/math.html - mathematical functions\nimport itertools\n# https://docs.python.org/3/library/itertools.html - permutations and combinations\nimport random\n# https://docs.python.org/3/library/random.html -generating random samples\nimport numpy as np\n# https://numpy.org/doc/stable/ - numerical computing for series of values\nimport matplotlib\nimport matplotlib.pyplot as plt\n# https://matplotlib.org/stable/contents.html plotting \nimport seaborn as sns\n# https://seaborn.pydata.org/ -plotting\nimport pandas as pd\n# https://pandas.pydata.org/pandas-docs/stable/index.html - data manipulation and analysis\nimport scipy\nimport scipy.stats as stats\n# https://docs.scipy.org/doc/scipy/reference/stats.html - statistical functios\n\n# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.hypergeom.html -\nfrom collections import Counter\n# https://docs.python.org/3/library/collections.html#collections.Counter - counting hashable objects\nimport voila\n# https://voila.readthedocs.io/en/stable/ - turning Jupyter notebooks into standalone web applications\nimport ipywidgets as widgets\n# https://ipywidgets.readthedocs.io/en/stable/ - interactive HTML widgets for\n\nfrom ipywidgets import interact, interactive, HBox, VBox, HTML\n\nplt.rcParams[\"figure.figsize\"] = (6, 4)\nplt.rcParams[\"axes.grid\"] = True"
  },
  {
    "objectID": "notebooks/problems.html#problem-1---combinations-and-simulation",
    "href": "notebooks/problems.html#problem-1---combinations-and-simulation",
    "title": "Problems",
    "section": "Problem 1 - Combinations and Simulation",
    "text": "Problem 1 - Combinations and Simulation\nTasks: 1. Calculating the probability of randomly selecting exactly the 3 cups with milk added first. 2. The simulation of the process 1,000 times to verify result.\nThe lady tasting tea is an experiment devised by British polymath Ronald Fisher in 1935. It was based on the claim of Fisher’s acquaintance, Muriel Bristol, who stated she could tell whether milk or tea was poured first into a cup. Fisher’s intention was not about tea, but about designing an experiment to test a claim fairly, minimizing the factor of randomness so that the claim could be proved or disproved on an objective basis.\nThe null hypothesis of the experiment is that the taster has no remarkable ability and is simply guessing at random. The alternative hypothesis is that the tester has the special ability she/he/they claims. The probability of guessing correctly at random is calculated and then compared to a statistical threshold (usually 5%). If the taster correctly recognised a number of cups that would be highly unlikely to be guessed at random ( usually with probability less than 5%), then the null hypothesis could be rejected.\nhttps://en.wikipedia.org/wiki/Lady_tasting_tea\n\nCalculation of the probability of selecting 0, 1, 2, 3 cups from 10 randomly\nFirst I need to count number of ways how to choose 3 cups from 10. Factorial number! is a number of ways how to arrange certain number of objects. Binomial coefficient formula gives us number of ways of selecting k objects from n objects without replacement and without order.\n\\[ \\binom{n}{k} = \\frac{n!}{k!(n-k)!} \\]\n\n# Using math.factorial\ntotal_cups=10\nspecial_cups=3\nways = math.factorial(total_cups) / (math.factorial(special_cups) * math.factorial(total_cups - special_cups))\nways\n\n120.0\n\n\n\ncups_list = list(range(total_cups))\ncups_list\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n# Or itertools.combinations\ncombs_cups = list(itertools.combinations(cups_list, special_cups))\nlen(combs_cups)\n\n120\n\n\nThere are 120 ways of choose 3 cups from 10 cups.\nCalculation of probability with hypergeometric formula\n\\[\nP(\\text{correct\\_cups}) =\n\\frac{\\binom{\\text{special\\_cups}}{\\text{correct\\_cups}} \\cdot \\binom{\\text{total\\_cups} - \\text{special\\_cups}}{\\text{cups\\_chosen} - \\text{correct\\_cups}}}\n{\\binom{\\text{total\\_cups}}{\\text{cups\\_chosen}}}\n\\]\nhttps://www.youtube.com/watch?v=uzN7U88KSx8\n\nprob_0 = (math.comb(3, 0) * math.comb(10 - 3, 3 - 0)) / math.comb(10, 3)\nprob_0 * 100\n\n29.166666666666668\n\n\n29,16 % probability of choosing no cup right.\n\nprob_1 = (math.comb(3, 1) * math.comb(10 - 3, 3 - 1)) / math.comb(10, 3)\nprob_1 * 100\n\n52.5\n\n\n52.5 % probability of choosing 1 cup right.\n\nprob_2 = (math.comb(3, 2) * math.comb(10 - 3, 3 - 2)) / math.comb(10, 3)\nprob_2 * 100\n\n17.5\n\n\n17.5 % probability of choosing 2 cups right.\n\nprob_3 = (math.comb(3, 3) * math.comb(10 - 3, 3 - 3)) / math.comb(10, 3)\nprob_3 * 100\n\n0.8333333333333334\n\n\n0.83 % probability of choosing all 3 cups right.\n\n\nSimulation\n\ntotal_cups = 10\nspecial_cups = 3\ncups_chosen = 3\ntrials = 1000\ncups_list = list(range(total_cups))\n\n# Simulation\nresults = []\nfor _ in range(trials):\n    # randomly choose the \"special\" cups\n    special_set = set(random.sample(cups_list, special_cups))\n    # randomly choose the guess\n    guess_set = set(random.sample(cups_list, cups_chosen))\n    # count overlap\n    overlap = len(special_set & guess_set)\n    results.append(overlap)\n\n\ncounts = Counter(results) # Count frequencies\ncounts_sorted = dict(sorted(counts.items()))# Sorting counts\n\nsimulated_probs = {k: counts[k] / trials for k in range(special_cups + 1)}\n#Print results\nprint(\"counts:\", counts_sorted)\nprint(\"simulated_probs:\", simulated_probs)\n\ncounts: {0: 321, 1: 495, 2: 178, 3: 6}\nsimulated_probs: {0: 0.321, 1: 0.495, 2: 0.178, 3: 0.006}\n\n\nAfter running this simulation 5 times, I got 5 different results in counts and probability. This is thanks to sampling error. As 3 cups with milk first and  3 cups guessed by the taster are chosen randomly, the probability fluctuates around 5 percent of the theoretical value.\nhttps://en.wikipedia.org/wiki/Sampling_error\nThe law of large number states that the average of the results obtained from a large number of independent random samples converges to the true value.\nhttps://en.wikipedia.org/wiki/Law_of_large_numbers\nI am going to increase number of trials to 1000000.\n\n# Simulation 1000000 trials\nresults_million = []\nfor _ in range(1000000):\n    # randomly choose the \"special\" cups\n    special_set = set(random.sample(cups_list, special_cups))\n    # randomly choose the guess\n    guess_set = set(random.sample(cups_list, cups_chosen))\n    # count overlap\n    overlap = len(special_set & guess_set)\n    results_million.append(overlap)\n\n\ncounts_million = Counter(results_million) # Count frequencies\ncounts_sorted_million = dict(sorted(counts_million.items()))# Sorting counts\n\nsimulated_probs_million = {k: counts_million[k] / 1000000 for k in range(special_cups + 1)}\n#Print results\nprint(\"counts:\", counts_sorted_million)\nprint(\"simulated_probs:\", simulated_probs_million)\n\ncounts: {0: 291611, 1: 525402, 2: 174859, 3: 8128}\nsimulated_probs: {0: 0.291611, 1: 0.525402, 2: 0.174859, 3: 0.008128}\n\n\n\n# Plot \nks = list(counts_sorted_million.keys())\nvalues = list(counts_sorted_million.values())\n# Labels and title\nplt.bar(ks, values, tick_label=[f\"{k} correct\" for k in ks], color='olive')\nplt.ylabel(\"Counts\")\nplt.title(f\"Simulation of 1,000,000 trials\")\n\n# labels above bars\nfor i, v in zip(ks, values):\n    plt.text(i, v + 5, str(v), ha='center')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# %%\ndef nCk(n, k):\n    return math.comb(n, k)\n\ndef hypergeom_pmf(k, N, K, n):\n    \"\"\"\n    k = number of special cups guessed correctly\n    N = total cups\n    K = special cups\n    n = cups chosen (guessed)\n    \"\"\"\n    if k &lt; 0 or k &gt; K or k &gt; n:\n        return 0\n    return nCk(K, k) * nCk(N - K, n - k) / nCk(N, n)\n\n\n# %%\ndef simulate_cups(total_cups=10, special_cups=3, cups_chosen=3, trials=5000):\n    # Basic sanity corrections\n    total_cups = max(1, total_cups)\n    special_cups = max(1, min(special_cups, total_cups))\n    cups_chosen = max(1, min(cups_chosen, total_cups))\n\n    cups_list = list(range(total_cups))\n\n    # Simulation\n    results = []\n    for _ in range(trials):\n        special_set = set(random.sample(cups_list, special_cups))\n        guess_set = set(random.sample(cups_list, cups_chosen))\n        overlap = len(special_set & guess_set)\n        results.append(overlap)\n\n    counts = Counter(results)\n    ks = np.arange(0, min(special_cups, cups_chosen) + 1)\n\n    simulated_probs = np.array([counts.get(k, 0) / trials for k in ks])\n    theoretical_probs = np.array([\n        hypergeom_pmf(k, total_cups, special_cups, cups_chosen) for k in ks\n    ])\n\n    fig, ax = plt.subplots()\n    width = 0.4\n    ax.bar(ks - width/2, theoretical_probs, width=width, label=\"Theoretical\")\n    ax.bar(ks + width/2, simulated_probs, width=width, alpha=0.7, label=\"Simulated\")\n\n    ax.set_xlabel(\"Number of correct special cups\")\n    ax.set_ylabel(\"Probability\")\n    ax.set_title(\"Hypergeometric vs simulation\")\n    ax.legend()\n    plt.show()\n\n    print(\"k\\tTheoretical\\tSimulated\")\n    for k, t, s in zip(ks, theoretical_probs, simulated_probs):\n        print(f\"{k}\\t{t:.4f}\\t\\t{s:.4f}\")\n\n\ntotal_cups_slider = widgets.IntSlider(\n    description=\"Total cups\",\n    min=3, max=30, step=1, value=10\n)\nspecial_cups_slider = widgets.IntSlider(\n    description=\"Special cups\",\n    min=1, max=30, step=1, value=3\n)\ncups_chosen_slider = widgets.IntSlider(\n    description=\"Cups chosen\",\n    min=1, max=10, step=1, value=3\n)\ntrials_slider = widgets.IntSlider(\n    description=\"Trials\",\n    min=1, max=1000000, step=100, value=5_000\n)\n\ncontrols = {\n    \"total_cups\": total_cups_slider,\n    \"special_cups\": special_cups_slider,\n    \"cups_chosen\": cups_chosen_slider,\n    \"trials\": trials_slider,\n}\n\nui = VBox([\n    HTML(\"&lt;h3&gt;Hypergeometric: correct guesses&lt;/h3&gt;\"),\n    HBox([total_cups_slider, special_cups_slider]),\n    HBox([cups_chosen_slider, trials_slider]),\n])\n\nout = widgets.Output()\n\ndef update_problem1(**kwargs):\n    with out:\n        out.clear_output(wait=True)\n        simulate_cups(**kwargs)\n\ninteractive_output_p1 = widgets.interactive_output(update_problem1, controls)\n\ndisplay(ui, out)"
  },
  {
    "objectID": "notebooks/problems.html#problem-2-normal-distribution",
    "href": "notebooks/problems.html#problem-2-normal-distribution",
    "title": "Problems",
    "section": "Problem 2: Normal Distribution",
    "text": "Problem 2: Normal Distribution\nTask: 1. Evaluating whether numpy.random.standard_normal() generates values from a true normal distribution by generating a sample of 100,000 values. 2. Using scipy.stats.shapiro() to test whether the sample comes from a normal distribution.\n3. Creating a histogram of sample and overlaying the probability density function (PDF) of the standard normal distribution on the histogram.\nA normal distribution or Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n\\[ f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\]\nThe parameter μ is the mean or expectation of the distribution (and also its median and mode), while the parameter \\(σ^2\\) is the variance. The standard deviation of the distribution is σ. A random variable with a Gaussian distribution is said to be normally distributed.\nhttps://en.wikipedia.org/wiki/Normal_distribution\n\nGenerating a sample of 100000 values\nhttps://numpy.org/doc/2.2/reference/random/generated/numpy.random.standard_normal.html#numpy.random.standard_normal\n\n# Generating the sample\ndata=np.random.standard_normal(100000)\ndata\n\narray([-1.61459526, -0.1570416 , -0.34386251, ..., -0.65692043,\n       -0.9060689 ,  0.71832454])\n\n\n\n\nShapiro-Wilk test\nThe Shapiro-Wilk test tests the null hypothesis that a data was drawn from a normal distribution. The W statistic is a measure of how close the data distribution is to a normal distribution. P-value measures the probability of observing the W value. If the p-value is greater than 0.05, it indicates that the data are probably normally distributed.\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html#shapiro\n\nstats.shapiro(data)\n\nc:\\Users\\Lucia\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:1816: UserWarning: p-value may not be accurate for N &gt; 5000.\n  warnings.warn(\"p-value may not be accurate for N &gt; 5000.\")\n\n\nShapiroResult(statistic=0.9999819397926331, pvalue=0.9522344470024109)\n\n\nIn the documentation for the Shapiro-Wilk test, it is noted that with a number of samples over 5000, the W-statistic is accurate, but the p-value may not be. It is given by the limitation of the software in the calculation of the p-value over 5000. There is also a strong indication that with a W-statistic as extreme as 0.99, the data are probably from a normal distribution.\nhttps://github.com/scipy/scipy/blob/v1.16.1/scipy/stats/_morestats.py#L1943-L2030\nI am going to use the Kolmogorov-Smirnov test, which could work with large numbers of samples, to prove the normal distribution of data. This test (when just one sample is tested) compares the data’s distribution to the theoretical distribution ( which could be normal, exponential, uniform, and so on). Then the D-statistic value is calculated. The range of the d-statistic is from 0 to 1. The closer the d-statistic value is to 0, the more similar the data’s distribution is to the theoretical distribution. P-value is a calculation of the probability of reaching a d-statistic value. The statistic location is the point of maximal difference between the theoretical distribution and the data’s distribution. The statistical sign shows the shift of data values to theoretical data values. 1 smaller values, -1 larger values than expected. The null hypothesis is that the sample’s distribution is not significantly different from the theoretical distribution when the p-value &gt; 0.05.\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html#kstest\n\nstats.kstest(data, 'norm')\n\nKstestResult(statistic=0.0020965560152093055, pvalue=0.7707894182949662, statistic_location=0.13111177310610866, statistic_sign=-1)\n\n\nI can assume that the data are normally distributed.\n\n\nHistogram and plot of normal probability density function\nA histogram is a plot of the distribution of values. https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html#matplotlib-pyplot-hist https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy-stats-norm\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Histogram\nax.hist(data, bins=100, color='olive', density=True, alpha=0.7)\n\n# X values for the curve\nx = np.linspace(-4, 4, 1000)\ny = stats.norm.pdf(x, loc=0, scale=1)\n\n# Standard normal PDF\nax.plot(x, y, lw=2, color='green',  alpha=0.7)  \n\n# 68-95-99.7 rule\nax.fill_between(x, y, where=(x &lt; -1) | (x &gt; 1), alpha=0.2, color='yellow')\nax.fill_between(x, y, where=(x &lt; -2) | (x &gt; 2), alpha=0.2, color='red')\n\n# Labels\nax.set_title(\"Data Histogram\", fontsize=16)\nax.set_ylabel(\"Density\")\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows a typical curve of data that is normally distributed - bell-shaped and symmetrical around the mean. About 68% of the data fall within 1 standard deviation of the mean, about 95% of the data fall within 2 standard deviations of the mean, and about 99.7% of the data fall within 3 standard deviations of the mean.\nhttps://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule#:~:text=In%20statistics%2C%20the%2068%E2%80%9395,sigma"
  },
  {
    "objectID": "notebooks/problems.html#problem-3-t-tests",
    "href": "notebooks/problems.html#problem-3-t-tests",
    "title": "Problems",
    "section": "Problem 3: T-Tests",
    "text": "Problem 3: T-Tests\nTasks: 1. Comparing the resting heart rates of smokers and non-smokers from sets containing resting heart rates for a sample of patients, along with whether they smoke or not. Calculating the t-statistic based on the data set, using Python without scipy or statsmodels. 2. Comparing it to the value given by scipy.stats.\n\nWhat is a t-test?\nA t-test is a statistical test that compares means to see if the difference is likely due to chance or reflects a real effect in the population. It can be done as a one-sample t-test (when the mean of the sample is compared with the hypothesized mean) or as a two-sample t-test with related samples (paired t-test) or unrelated samples (independent t-test). The t-statistic measures how far the sample result (the difference in means) is from what the null hypothesis expects, in units of standard error (SE).​\nWhen equal variances are assumed (pooled t-test)\n\\[ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p \\cdot \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\]\n\\[ SE = s_p \\cdot \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} \\]\nwhere  \n\\[ s_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}} \\]\n$ n_1, n_2 $ are sample sizes\n$ s_p $ is the pooled standard deviation\n$ s_1^2, s_2^2 $ are sample variances\n$ {X}_1, {X}_2 $ are sample means\nThe null hypothesis of the t-test is that there is no significant difference in the means of two groups, while p-value &gt; 0.05, so a difference between the means could be given by random chance.\n\n\nData\nPatient ID Smokes Resting Heart Rate\n0    Yes    81\n1    No    68\n2    Yes    69\n3    Yes    76\n4    No    74\n5    Yes    77\n6    Yes    79\n7    No    75\n8    Yes    61\n9    No    68\n10    Yes    74\n11    No    72\n12    Yes    73\n13    Yes    70\n14    No    67\n15    No    67\nAs data to compare are unpaired samples, I chose independent t-test.\n\n# Arrays \nsmokers = [81, 69, 76, 77, 79, 61, 74, 73, 70]\nnonsmokers = [68, 74, 75, 68, 72, 67, 67]\n\n\n\nAssumptions\nThere are 6 assumptions that the independent t-test should meet:\nhttps://statistics.laerd.com/spss-tutorials/independent-t-test-using-spss-statistics.php 1. The dependent variable should be measured on a continuous scale - O.K. in our case. 2. The independent variable should consist of two categorical, independent groups - O.K. in our case. 3. Independence of observations, which means that there is no relationship between the observations in each group or between the groups themselves - O.K. in our case. 4. There should be no significant outliers - It will be checked. 5. The dependent variable should be approximately normally distributed for each group of the independent variable - It will be checked. 6. Homogeneity of variances - It will be checked\n\nInterquartile range method to find outliers\nIn this method, median, first interquartile (Q1), and third interquartile (Q3) values have to be identified. Q3 - Q1 is giving the interquartile range (IQR). Then, lower and upper fence values can be calculated as:\nUpper fence = Q3 + (1.5 * IQR)\nLower fence = Q1 - (1.5 * IQR)\nThe significant outliers are any values greater than the upper fence value or less than the lower fence value.\nhttps://en.wikipedia.org/wiki/Interquartile_range\n\n# sorting helps to identify median, Q1, Q3 values\nsmokers_sorted = sorted(smokers)\nnonsmokers_sorted = sorted(nonsmokers)\n\nprint(\"Smokers sorted:\", smokers_sorted)\nprint(\"Non-smokers sorted:\", nonsmokers_sorted)\n\nSmokers sorted: [61, 69, 70, 73, 74, 76, 77, 79, 81]\nNon-smokers sorted: [67, 67, 68, 68, 72, 74, 75]\n\n\n\nsmokers_M = 74                        \nsmokers_Q1 = 70                                                  \nsmokers_Q3 = 77\nsmokers_IQR = smokers_Q3 - smokers_Q1 \n\nlow_fence_smokers= smokers_Q1 - (1.5 * smokers_IQR)\nhigh_fence_smokers = smokers_Q3 + (1.5 * smokers_IQR)\n\nprint(\"Low fence for smokers: \",low_fence_smokers)\nprint(\"High fence for smokers: \",high_fence_smokers)\nprint(\"Smokers data: \", smokers_sorted)\n\nLow fence for smokers:  59.5\nHigh fence for smokers:  87.5\nSmokers data:  [61, 69, 70, 73, 74, 76, 77, 79, 81]\n\n\nNo significant outliers for smokers array can be found.\n\nnonsmokers_M = 68                        \nnonsmokers_Q1 = 67.5                                                  \nnonsmokers_Q3 = 73\nnonsmokers_IQR = nonsmokers_Q3 - nonsmokers_Q1 \n\nlow_fence_nonsmokers= nonsmokers_Q1 - (1.5 * nonsmokers_IQR)\nhigh_fence_nonsmokers = nonsmokers_Q3 + (1.5 * nonsmokers_IQR)\n\nprint(\"Low fence for nonsmokers: \",low_fence_nonsmokers)\nprint(\"High fence for smokers: \",high_fence_nonsmokers)\nprint(\"Smokers data: \", nonsmokers_sorted)\n\nLow fence for nonsmokers:  59.25\nHigh fence for smokers:  81.25\nSmokers data:  [67, 67, 68, 68, 72, 74, 75]\n\n\nNo significant outlier for nonsmoker array can be found.\n\n\nNormal distribution check\nI decided to check the normal distribution of values with the Shapiro-Wilk test, as this method works well for small datasets. The minimum number of samples for Shapiro-Wilk test is 3.\n\nstats.shapiro(smokers)\n\nShapiroResult(statistic=0.9473088979721069, pvalue=0.6605566143989563)\n\n\n\nstats.shapiro(nonsmokers)\n\nShapiroResult(statistic=0.8303568959236145, pvalue=0.08064436167478561)\n\n\nI can assume that data in both arrays are normally distributed as p-value in both test is not higher than 0.05.\n\n\nHomogeneity of variances\nVariance measures how the values of the set are spread around the mean.   A small variance → data points are close to the mean.\nA large variance → data points are more spread out.\nA very easy way to compare the variances of datasets is using a box plot. If the lengths of whiskers differ greatly, the data in the datasets are not spread similarly.\nhttps://seaborn.pydata.org/generated/seaborn.boxplot.html\n\nsns.boxplot(data=(smokers, nonsmokers), palette=\"Set3\")\n\n\n\n\n\n\n\n\nVariance formula: \\[\ns^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n - 1}\n\\] where $ x_1 $ is 1 value in group(array,set).\n\n# Calculate variance of smokers array\nmean_smokers = sum(smokers) / len(smokers)\nvariance_smokers = sum((x - mean_smokers) ** 2 for x in smokers) / (len(smokers) - 1) # x is each value in array\nprint(\"Variance of smokers:\", variance_smokers)\n\n# Calculate variance of nonsmokers array\nmean_nonsmokers = sum(nonsmokers) / len(nonsmokers)\nvariance_nonsmokers = sum((x - mean_nonsmokers) ** 2 for x in nonsmokers) / (len(nonsmokers) - 1) # x is each value in array\nprint(\"Variance of nonsmokers:\", variance_nonsmokers)\n\nVariance of smokers: 36.75\nVariance of nonsmokers: 11.809523809523808\n\n\nThese two arrays do not have equal variances.\n\n\nCalculating t-statistic without stats.scipy\nAs variances of samples are not equal, I have to use a bit Welch’s method of calculating t-statistic \\[ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\tfrac{s_1^2}{n_1} + \\tfrac{s_2^2}{n_2}}} \\]\nor\n\\[ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{SE} \\]\nhttps://en.wikipedia.org/wiki/Welch%27s_t-test\n\nn1 = len(smokers)\nn2 = len(nonsmokers)\nse = ((variance_smokers / n1) + (variance_nonsmokers / n2)) ** 0.5\n\n# Calculating t-statistic without stats.scipy\nt_value = (mean_smokers - mean_nonsmokers) / se\nprint(\"Calculated t-value (Welch's t-test): \", t_value)\nprint(\"Means of smokers' group:\", mean_smokers)\nprint(\"Means of non-smokers' group:\", mean_nonsmokers)\n\nCalculated t-value (Welch's t-test):  1.328165641371784\nMeans of smokers' group: 73.33333333333333\nMeans of non-smokers' group: 70.14285714285714\n\n\n\n# Welch's t-Test\nstats.ttest_ind(smokers, nonsmokers, equal_var = False)\n\nTtest_indResult(statistic=1.328165641371784, pvalue=0.20694631459381652)\n\n\nI can assume that, even though there is a difference in means between the smokers’ and nonsmokers’ groups, this difference is not significant."
  },
  {
    "objectID": "notebooks/problems.html#problem-4-type-i-error",
    "href": "notebooks/problems.html#problem-4-type-i-error",
    "title": "Problems",
    "section": "Problem 4: Type I Error",
    "text": "Problem 4: Type I Error\nTasks: 1. Generating three samples with 100 values each with numpy.random.standard_normal() 2. Performing one-way ANOVA on the three samples and counting whenever a Type I error occurs.\n\nType I error\nA Type I error happens when the null hypothesis is rejected even though it is actually true. For example, an alternative cure is tested. By chance, in a small number of trials, it may appear effective, even though in reality it has no true effect. If the significance level is at 5% (𝛼 = 0.05), this means that in about 5% of studies, it might falsely conclude the cure works when it does not. This is called a false positive error.\nhttps://en.wikipedia.org/wiki/Type_I_and_type_II_errors\n\n\nANOVA\nANOVA is a statistical test, with at least 2 groups, when groups’ means are compared. ANOVA test checks the ratio:\n\\[ F = \\frac {Between-group variance}{Within-group variance} \\]\nIf this ratio is much larger than 1, it suggests that the group means are not all equal. The null hypothesis of ANOVA is: All groups’ means are equal. The alternative hypothesis: At least 1 mean is different. P-value is the probability of obtaining such a t-statistic value. If \\(p \\leq 0.05\\) the alternative hypothesis could be assumed as right.\nhttps://en.wikipedia.org/wiki/Analysis_of_variance\n\n\nSimulation\n\n\nno_type_i = 0\nalpha = 0.05\nn_sim = 10000\n\nfor _ in range(n_sim):\n    # Generating 3 samples\n    sample_a = np.random.standard_normal(100)\n    sample_b= np.random.standard_normal(100)\n    sample_c= np.random.standard_normal(100)\n    \n    # one-way ANOVA\n    f_stat, p_value = stats.f_oneway(sample_a, sample_b, sample_c)\n    \n    # Counting type I errors\n    if p_value &lt; alpha:\n        no_type_i += 1\n\n# Probability\nprob_type_i = no_type_i / n_sim\nprint(f\"Type I Error Rate: {prob_type_i:}\")\n\nType I Error Rate: 0.0448\n\n\nThis simulation is continuously giving a type I error estimated probability of around 0.05. What if I change the number of trials to 100?\n\n# restart and run dependencies at the beginning of the notbook first\nno_type_i = 0\nalpha = 0.05\nn_sim = 100\n\nfor _ in range(n_sim):\n    # Generating 3 samples\n    sample_a = np.random.standard_normal(100)\n    sample_b= np.random.standard_normal(100)\n    sample_c= np.random.standard_normal(100)\n    \n    # one-way ANOVA\n    f_stat, p_value = stats.f_oneway(sample_a, sample_b, sample_c)\n    \n    # Counting type I errors\n    if p_value &lt; alpha:\n        no_type_i += 1\n\n# Probability\nprob_type_i = no_type_i / n_sim\nprint(f\"Type I Error Rate: {prob_type_i:}\")\n\nType I Error Rate: 0.04\n\n\nThe Type I error rate fluctuates more. This is thanks to the Law of large numbers that I mentioned above when I explained the first problem - Combinations and Simulation. This difference can be counted as Monte Carlo standard error\nMCSE = np.sqrt(alpha * (1 - alpha) / number of trials)\nhttps://en.wikipedia.org/wiki/Monte_Carlo_method\n\n\nmcse100 = np.sqrt(alpha * (1 - alpha) / 100)\nprint(f\"Approx MC SE: {mcse100:} )\")\n\nApprox MC SE: 0.021794494717703367 )\n\n\n\nmcse10000 = np.sqrt(alpha * (1 - alpha) / 10000)\nprint(f\"Approx MC SE: {mcse10000:} )\")\n\nApprox MC SE: 0.0021794494717703367 )"
  },
  {
    "objectID": "notebooks/problems.html#problem-5-binomial-distribution",
    "href": "notebooks/problems.html#problem-5-binomial-distribution",
    "title": "Problems",
    "section": "Problem 5: Binomial distribution",
    "text": "Problem 5: Binomial distribution\n\nWhat is the binomial distribution?\n“The binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes-no question, and each with its own Boolean-valued outcome: success (with probability p) or failure (with probability q = 1 − p).”\nhttps://en.wikipedia.org/wiki/Binomial_distribution\nThe term “discrete” describes situations where a random variable can take on specific, separate values (not any value in between).  In comparison with the normal distribution (which is theoretically infinite), the binomial distribution is finite. Examples of experiments where data are distributed binomially could be coin flips or guessing the right answers in a test randomly.\nThe probability mass function (PMF) of the binomial distribution:\n\\[\nf(k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, 2, \\dots, n\n\\]\nwhere\nn = Total number of trials\nk = Number of successes\np = Probability of success in the range 0 ≤ p ≤ 1\nFor generating samples from a binomial distribution, I am going to use numpy.random.binomial(). https://numpy.org/doc/2.0/reference/random/generated/numpy.random.binomial.html#numpy-random-binomial\n\n# Simulation: a person guesses 100 yes/no questions\nguess_test1 = np.random.binomial(n=1, p=0.5, size=100)\nprint(guess_test1)\n\n[0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0\n 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1\n 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1]\n\n\n\n# Now simulation of 1000 people guessing 1000 yes/no questions\nn_questions = 1000   # number of yes/no questions\np = 0.5             # probability of correct guess\nn_trials = 1000  # simulate 1000 times\n# Generating data\nscores = np.random.binomial(n_questions, p, size=n_trials)\n\n# plot histogram\nplt.hist(scores, bins=1000, edgecolor='black', color='blue')\nplt.xlabel('Number of correct guesses')\nplt.ylabel('Frequency')\nplt.title('Distribution of correct guesses ( 1000 simulations)')\nplt.show()\n\n\n\n\n\n\n\n\nThis plot resembles the plot of a normal distribution. I am going to check this with the Shapiro-Wilk test.\n\nnormal_bool=stats.shapiro(scores)\nnormal_bool\n\nShapiroResult(statistic=0.9972940683364868, pvalue=0.09280858933925629)\n\n\nThe Shapiro-Wilk test gave me proof of normally distributed data, even though I cannot assume that this data is normally distributed. “In probability theory, the central limit theorem (CLT) states that, under appropriate conditions, the distribution of a normalized version of the sample mean converges to a standard normal distribution.” https://en.wikipedia.org/wiki/Central_limit_theorem\nIn the case of the binomial distribution, these conditions are p-value = 0.5 and number of samples n(1−p) ≥ 5.\n\n# Simulation of 50 people guessing 20 questions, each with 4 options, only 1 is correct\nquestions4 = 20   # number of questions\np4 = 0.25             # 1 from 4 answers is correct\ntrials4 = 50 # simulate 50 times\n# Generating data\nscores4 = np.random.binomial(questions4, p4, trials4)\n\n\n# plot histogram\nplt.hist(scores4, bins=50, edgecolor='black', alpha=0.7, color='blue')\nplt.xlabel('Number of correct guesses')\nplt.ylabel('Frequency')\nplt.title('Distribution of correct guesses ( 50 simulations)')\nplt.show()\n\n\n\n\n\n\n\n\n\nnormal_bool4=stats.shapiro(scores4)\nnormal_bool4\n\nShapiroResult(statistic=0.963204026222229, pvalue=0.12109135091304779)\n\n\nBased on the Shapiro-Wilk test values of statistic and p, I can assume that this data does not come from the normal distribution."
  },
  {
    "objectID": "notebooks/problems.html#end",
    "href": "notebooks/problems.html#end",
    "title": "Problems",
    "section": "End",
    "text": "End"
  }
]